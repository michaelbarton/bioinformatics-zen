---
  kind: article
  title: "A problem in bioinformatics: we often don't know what we want."
  feed: true
  date: "2013-02-27 08:00:00"
---

Biology graduate students and researchers in biology ask a variant of the same
question every day: which software is the best for analysing my data? This
might be assembling sequence data into a complete genome, doing genome-wide
associate studies, or any of the hundreds of other steps needed to be done as
part of writing a paper or thesis. I argue that this problem is not that no one
agrees on which single tool is the best, but instead no one even knows what
makes one tool better than another.

For any exploratory analysis you might be interested in, there is rarely any
single pass or fail criteria. Instead there are a 1000+ different metrics than
can be collected on a biological data set. As an example for this article,
after sequencing a genome, what would it mean for one assembly of the genome to
be better than another? Should the contiguity (NG50) should be as high as
possible, and if so how many misassemblies or incorrect genes can be tolerated
for better contiguity?

The purpose of sequencing a genome might be to get the parts that are useful
for downstream anaylsis, such as protein-coding regions. Perhaps if all the
protein-coding regions as best as possible then the rest of the intergenic and
non coding regions might be less important. Would we care if there was less
contiguity for the sake 100% accurate protein-genes. If the answer is no, then
what should the percentage accuracy of coding to non-coding regions be, 5%/95%?
What would those percentages even be measured as?

Assume the case where a genome assembler produced a perfect genome assembly. We
would compare this perfect assembly with a gold standard and get exactly what
we expected. A single contig with no mistakes or errors in it, while at the
same time we don't care about nucleotide polymorphism or population data. This
then frames this as a binary choice: either we get this single perfect result
with no mistakes or we get a assembly that has some mistakes in it and we have
to decide what kinds of mistakes we're comfortable with and which ones we're
not.

Once we have to accept we're unlikely to get a single perfect assembly we'll
have to make some trade offs around what types of errors we'll accept, and what
they'll look like. For example we might say, I'm willing to accept a little bit
of misassembly in exchange for more contiguity, with mostly correct gene
sequences.

This gets more to the point of what we care about, but we have to drill down
into even more detail. What do we mean when we say contiguity maybe NG50? What
about misassemblies? Is it the size or the number of missamblies? Would you
happier with one giant misassembly or hundreds of little ones. When you say
mostly correct coding regions, do you mean that 95% of the genes are 100%
accurate, or 100% of the genes should be at least 95% accurate. By the way how
much do you care about rRNA genes?

But to choose whether one assembler is better than another we have to quantify
this exactly. We have to be able to say my function for comparing one assembler
with another is something like:

  * 70% * The inverse of the number of incorrect bases across all coding genes
    divided by the total number of bases across all coding genes.

  * 10% * The inverse of the number of incorrect bases in the rRNA genes.
    divided by the total number of bases across rRNA coding genes.

  * 10% * The NG50 of the genome divided by the length of the genome.

  * 5% * The number of bases in misassembled regions divided by the length
    of the genome

  * 5% * The number of misassembly regions divided by the length of the
    genome

Adding up all these figures together should return a metric between 0-1 that
when comparing two assemblies should tell you which one is better than the
other for a single metric. With this metric, you could then write a simple
script that would test all the different assemblers and tell you which one is
the best.

Fine when just looking at single genome assembly. What if you had to compare
two genomes? We might repeat this process for every single genome we looked at.

If this was infeasible or intractable because of the computational costs or
because we didn't want to spend all of our time doing this, we would have to
instead perform this analysis for representative sample and then extrapolate
for all the data we might eventually look at.

Once we do any kind extrapolation, then the question includes a statisitical
component. Do we just want the assembler that produces the best by mean? What
if one assembler has a higher mean but also a wider variance.

In cases like this we'll have to make decisions around best on average or
produces a the least worst result.

# Summary

Should ideally frame the assembly in terms of a downstream objective that we
can more measurably care about. For example we are generally sequencing a
genome for the sake of doing something with that genome sequences.

Maybe using the genomes to identify genotypic differences between environments.

Unfortunately we the objective functin we optmise around is being the first to
publish in a high as ranked journal as possible.

When I tell you one assembler is better than another, you can be assured that I
have almost no idea what I'm talking about, and it's almost definitely not what
you had in mind either.
