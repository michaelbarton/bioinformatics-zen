---
  kind: article
  feed: true
  title: Categorising microbial genome assemblies using contiguity
  date: "2017-04-15 00:00 GMT"
---

Several years ago when I first started doing a post-doc in microbial genomics,
one of the first things I had to decide was what assembly software to use on
our sequencing data. The process I used at the time was to search on pubmed for
genome assembler papers, then try the subset of those that I could successfully
compile and run on my laptop. A better approach might have been to look, or ask
a question on the well-informed community site [seqanswers][seq].

[seq]: http://seqanswers.com/

Regardless of the approach, the underlying problem is the same where may PhD
students and post-doctoral researchers are unclear as to which is the best
software to use for their sequence data. What is the best way to then evaluate
if their results are any good or not?

The Joint Genome Institute suffers from the same problem, with a much larger
scale. We've sequenced thousands of microbial genomes in the last five years
and in each case we want to provide the best assembly given the available
assembly software. Over the last few years we have taken a two-pronged approach
to resolving this question. This is:

  * Create Docker images with standardised interfaces to the majority of recent
    genome assembly software. This is the [bioboxes project][bioboxes] in
    conjuction with [Peter Belmann][peter] and the other CAMI collaborators.

  * Exhaustively test all genome assembly software on real Illumina microbial
    sequencing data where we have a high quality reference genome we can
    benchmark the assembly against.

[bioboxes]: http://bioboxes.org/
[peter]: https://twitter.com/_pbelmann_

I write exhaustively because the total running time of this approach required
80 days of running time, or 411 days of total CPU time if we were to run each
assembler in serial. This produced 516 million contigs, totalling 233 gigabases
of FASTA sequence. Collecting all these contigs then allows us to benchmark
each of these assemblies by comparison to the high-quality reference sequence
using [QUAST][] In this instance when I write "high-quality" I specifically
mean either a pacbio assembly or a previously manually curated genome assembly.
For the rest of this post I will use a subset of metrics to quantify these
generated assemblies. These are the n\*50 and l\*50, specifically n50, ng50,
na50, nga50, l50, lg50, la50, and lga50.

## Contiguity metrics

[QUAST]: http://quast.sourceforge.net/quast

Many scientists have impaled themselves on the spike of attempting to explain
these esoteric metrics. I must unfortunately similarly skewer myself because
understanding the meaning of each of these is essential to understanding the
rest of this post. Therefore I have created the below drawing with the aim of
illustrating this.

TODO: insert drawing

The n50 metric[^n50] is then calculated in the following steps, where each
numbered point corresponds to same numbered point in the figure.

  1. Order all the contigs in the assembly by their length.

  2. Calculate a running cumulative sum of the contig lengths

  3. Find the point in the cumulative sum which half the total sum. In this
     figure the sum of contig lengths is 19, and the halfway point is therefore
     9.5.

  4. Pick the first contig that is greater than (specifically not greater then
     or equal to) this halfway point in this running cummulative sum. The
     length of this contig is the value returned as the calculated n50 metric.

This is the basic n50 metric. The other n\*50 metrics are updates to this basic
calculation:

  * ng50: Instead of using the halfway of point of the cumulative sum to find
    the pivot in the assembly, use half the sum of the reference the assembly
    if it is available. This is to prevent extraneous or duplicated contigs
    falsely inflating the n50 value. [^ng50]

  * na50: Before calculating n50, select only the contigs that align to the
    reference genome. This prevents falsely inflating the n50 value with
    contigs that should not be part of the assembly, such as contaminants or
    missamblies. [^nga50]

  * nga50: a combination of both of the above. Select only the contigs that
    align to the reference, and use half the sum of the reference contigs as
    the pivot point. This is the current strictest method of calculating
    n50.[^nga50]

The l\*50 are calculated with exactly the same method as outlined, with the
exception that instead of returning the length of the contig greater than the
pivot, it's index is returned. In the example above, using starting at 1 with
right-most contig count until the first contig greater than the pivot. This is
the l50 metric. All the n\*50 variants described above, apply the same way to
the corresponding l\*50 metrics.

I won't go into any further into the background of these metrics. Keith Bradnam
has a more detailed description about [why the n50 metrics are
useful][why-n50].

[why-n50]: http://www.acgt.me/blog/2013/7/8/why-is-n50-used-as-an-assembly-metric.html

## Scaling contiguity metrics

I'm going to define one further metric based on these previously described:
n\*s50. Any n\*s50 metric is the corresponding n\*50 metric divided by the sum
of the reference contigs. The reason for doing is as follows: imagine two
genomes _A_ and _B_, the length genome _A_ is 100Mbp and the length genome _B_
is 10Mbp. The lengths of the contigs for the genome assembly _A'_ are {40, 30,
20, 10} and the lengths of the contigs for genome assembly _B'_ are {4, 3, 2,
1}. The the ng50 for _A'_ is 40 while the ng50 for _B'_ is 4.

This example is to illustrate when comparing two _different_ genomes, the size
of the genome is a component in the ng50 value. We can say that _A'_ is a
better assembly than _B'_ because of the ng50 is higher for _A'_ but we know
that this is simply an artefact of the original genome size. Dividing both by
the reference size returns ngs50 values of 0.4 for both.

This is an important because when comparing genome assemblers we want to
compare across a variety of different genomes of unequal sizes. The signal we
wish to identify in how well an assembler does versus another should not be a
factor of the genome size. Converting n\*50 metrics into n\*s50 metrics allows
assemblies from two different genomes to be compared because because these
metrics are now invariant of the original genome size. [^lg50]

## Categorising genome assemblies

Given all metrics for across ~3300 different genome assemblies, the next
obvious step forward is to determine if there are any obvious trends in
assembly quality. The figure belows shows the first two principle components
using the calculated contiguity metrics. Each of the variables was scaled and
centred prior to calculating the PCA. [^scaling]

The loadings of each of the contiguity metrics are shown as the red arrows on
the plot. If you're unfamiliar with principle components analysis, the length
of each arrow shows how much variance is explained, and the direction of the
arrow shows how much they correlate with the other loadings.

# TODO: Insert PCA dotplot

The clearest result from this plot is the orthonganol relationship between the
l50, la50, lg50 ad lga50 metrics and the n50, ng50, na50, nga50 metrics. I.e.
the generated assemblies can be discriminated using the variance explained by
these two groups of metrics.

A further observation is that l50 and la50, lg50 and lga50 both appear strongly
correlated. The reason that all four of these metrics are not correlated likely
being in the difference in using the assembly or reference length as the pivot
in the calculation. Interestingly there is not the same difference between n50,
na50, ng50, nga50. I believe this may be a variant of the pigeon hole priciple
because l50 is a discrete random variable and unrelated genome assemblies are
likely to have the same value by chance simply because of the number of values
l50 will usually take. The choice of the pivot could therefore have a dramatic
larger effects on this at smaller values of lg50. N50 on the otherhand with
very large numbers in the 10s of thousands is essentially a continuous random
variable where the effect of using the assembly or the reference as a pivot
will have a much smaller effect on the calculated score.

# TODO: Insert PCA dotplot

The above figure shows the cumulative amount of variance explained by the
principle components. The dashed line shows 85% variance explained, the value
conventionally used to which components are use for further analysis.




assemblies cluster together using the mclust model based clustering
software[^clust]. The visualisation is generating by embedding the assemblies
in two dimension using t-Distributed Stochastic Neighbor Embedding
(t-SNE)[^tsne].

# TODO: Insert cluster plot


## Notes

[^n50]: [Keith Bradnam believes][keith] the first occurence of n50 was in the [human genome paper][human].

[keith]: http://www.acgt.me/blog/2015/6/11/l50-vs-n50-thats-another-fine-mess-that-bioinformatics-got-us-into
[human]: http://www.nature.com/nature/journal/v409/n6822/full/409860a0.html

[^ng50]: The ng50 metric was first defined in the [assemblathon 1 paper][asm1].

[asm1]: http://genome.cshlp.org/content/21/12/2224

[^nga50]: I believe both na50 and nga50 were defined by the team behind [QUAST][].

[^lg50]: The l\*50 metrics are invariant of size since they are calculated as the number of contigs. I think it could be argued that larger genomes are likely to have more difficult regions to assemble, therefore resulting in a higher l\*50 value. My unproven intuition however is that dividing l50 would introduce a much greater reference length bias than simply leaving the metric as is. An additional point is that ns50 is a scale-free metric, as is l50 and therefore makes these metrics comparable also.

[^clust]: Scrucca L, Fop M, Murphy TB, Raftery AE. mclust 5: Clustering, Classification and Density Estimation Using Gaussian Finite Mixture Models. The R journal. 2016;8(1):289-317.

[^tsne]: L.J.P. van der Maaten and G.E. Hinton. Visualizing High-Dimensional Data Using t-SNE. Journal of Machine Learning Research 9(Nov):2579-2605, 2008.

[^scaling]: PCA projects into dimensions maximising variance so scaling prevents the magnitute of the variable being the main component. Converting all the contiguity metrics into their z-score values is therefore the same as performing PCA on the correlation rather than covariance matrix. See this [question on cross validated for more details](http://stats.stackexchange.com/questions/53/pca-on-correlation-or-covariance).


































